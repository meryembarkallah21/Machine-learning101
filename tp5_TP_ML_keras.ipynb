{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsgL7f48wB4W"
      },
      "outputs": [],
      "source": [
        "import keras #pour construire les modele de reseau neurones\n",
        "from keras.datasets import mnist #pour importer dataset\n",
        "from keras.models import Sequential #pour preparer lemplacement des couches\n",
        "from keras.layers import Dense, Dropout #dense=fully connected\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam #2 fcts utilis√© pour la mise a jour des ports\n",
        "import matplotlib.pyplot as plt #pour dessiner les courbes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "taille entree = 28*28\n",
        "\n",
        "\n",
        "taille sortie = 10 (nbr des classes)"
      ],
      "metadata": {
        "id": "ZVnhFcfSzjlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikS7d472ykDm"
      },
      "outputs": [],
      "source": [
        "batch_size = 2048 #nbr des donnes d'entree pourune mise a jour de port\n",
        "num_classes = 10\n",
        "epochs = 198\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt9lWnS3yxj_"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()  #importer la dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "ZwxfLEZby25f",
        "outputId": "a8817610-700b-4892-c49e-356877d3ed0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(60000,)\n",
            "(10000,)\n",
            "[5 0 4 1 9 2 1 3 1 4]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efbc765d360>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(x_train.shape)   #nbr images , ligne , colonnes\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_train[:10])\n",
        "\n",
        "plt.imshow(x_train[1,:,:], cmap=\"gray\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-JL6Q8CzWnv",
        "outputId": "b4d4eb26-a070-499e-d71a-a14295cbea4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "x_train = x_train.astype('float32') # by default, images are stored as unsigned integers from 0 to 255 (you only need 8 bits)\n",
        "x_test = x_test.astype('float32') # by default, all training is done on \"float32\"\n",
        "\n",
        "x_train /= 255.0 #normalisation entre 0 et 1\n",
        "x_test /= 255.0\n",
        "\n",
        "print(x_train.shape[0], \"train samples\") # print the first entry of the size\n",
        "print(x_test.shape[0], \"test samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSIVRvcSzkKn",
        "outputId": "3301d332-0593-4969-9f17-7a05bc3d4960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# one-hot encoding: convert class vectors to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes) #La ligne suivante effectue une conversion des √©tiquettes de classe y_test en vecteurs binaires √† l'aide de la fonction to_categorical du module keras.utils :\n",
        "\n",
        "#sortie desir√© = par machine\n",
        "#sortie calculer = par expert\n",
        "\n",
        "print(y_train[1,:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIQqBj9L1Pu5",
        "outputId": "09602f7b-f3e8-4450-daf9-a618df7466e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 100)               78500     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 99,710\n",
            "Trainable params: 99,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential() #reservation\n",
        "model.add(Dense(100, activation='relu', input_shape=(784,)))  # Input Layer of 784 followed by a hidden layer of 30 neurons. (premiere couche cach√©e) (relu a une formule)\n",
        "model.add(Dropout(0.4))  # Adding a dropout layer to prevent overfitting. (desactivation de 40% de neurons de la la premiere couche cach√©e lors de l'apprentissage )\n",
        "model.add(Dense(100, activation='relu'))  # Adding more layers.\n",
        "model.add(Dropout(0.9))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer of 10 neurons for 10 classes.  (la derniere couche) (fonction softmax pour passage probabilit√©)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOdruXpkBtP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YruT97wu1R4I"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])  #optimiser responsable m√†j du poids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOi7psij1X2I",
        "outputId": "a640e89e-450e-45ac-c2ea-d7be1c31e14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/198\n",
            "30/30 [==============================] - 3s 75ms/step - loss: 2.2408 - accuracy: 0.1707 - val_loss: 1.8661 - val_accuracy: 0.5631\n",
            "Epoch 2/198\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 1.7441 - accuracy: 0.3697 - val_loss: 0.9611 - val_accuracy: 0.7240\n",
            "Epoch 3/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 1.3398 - accuracy: 0.5045 - val_loss: 0.6846 - val_accuracy: 0.8091\n",
            "Epoch 4/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 1.1275 - accuracy: 0.5847 - val_loss: 0.5325 - val_accuracy: 0.8797\n",
            "Epoch 5/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.9985 - accuracy: 0.6379 - val_loss: 0.4486 - val_accuracy: 0.8994\n",
            "Epoch 6/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.9131 - accuracy: 0.6725 - val_loss: 0.3841 - val_accuracy: 0.9076\n",
            "Epoch 7/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.8378 - accuracy: 0.7041 - val_loss: 0.3405 - val_accuracy: 0.9158\n",
            "Epoch 8/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.7844 - accuracy: 0.7299 - val_loss: 0.3151 - val_accuracy: 0.9153\n",
            "Epoch 9/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.7371 - accuracy: 0.7493 - val_loss: 0.2972 - val_accuracy: 0.9170\n",
            "Epoch 10/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.7092 - accuracy: 0.7586 - val_loss: 0.2855 - val_accuracy: 0.9187\n",
            "Epoch 11/198\n",
            "30/30 [==============================] - 2s 58ms/step - loss: 0.6764 - accuracy: 0.7723 - val_loss: 0.2745 - val_accuracy: 0.9245\n",
            "Epoch 12/198\n",
            "30/30 [==============================] - 2s 65ms/step - loss: 0.6527 - accuracy: 0.7819 - val_loss: 0.2639 - val_accuracy: 0.9252\n",
            "Epoch 13/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.6308 - accuracy: 0.7903 - val_loss: 0.2579 - val_accuracy: 0.9262\n",
            "Epoch 14/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.6134 - accuracy: 0.7976 - val_loss: 0.2515 - val_accuracy: 0.9270\n",
            "Epoch 15/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.5911 - accuracy: 0.8028 - val_loss: 0.2419 - val_accuracy: 0.9292\n",
            "Epoch 16/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.5786 - accuracy: 0.8103 - val_loss: 0.2357 - val_accuracy: 0.9321\n",
            "Epoch 17/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.5622 - accuracy: 0.8153 - val_loss: 0.2370 - val_accuracy: 0.9290\n",
            "Epoch 18/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.5431 - accuracy: 0.8244 - val_loss: 0.2371 - val_accuracy: 0.9289\n",
            "Epoch 19/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.5380 - accuracy: 0.8245 - val_loss: 0.2331 - val_accuracy: 0.9316\n",
            "Epoch 20/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.5267 - accuracy: 0.8274 - val_loss: 0.2418 - val_accuracy: 0.9253\n",
            "Epoch 21/198\n",
            "30/30 [==============================] - 1s 50ms/step - loss: 0.5181 - accuracy: 0.8322 - val_loss: 0.2312 - val_accuracy: 0.9296\n",
            "Epoch 22/198\n",
            "30/30 [==============================] - 2s 70ms/step - loss: 0.5020 - accuracy: 0.8373 - val_loss: 0.2411 - val_accuracy: 0.9241\n",
            "Epoch 23/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.5014 - accuracy: 0.8360 - val_loss: 0.2351 - val_accuracy: 0.9285\n",
            "Epoch 24/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.4886 - accuracy: 0.8399 - val_loss: 0.2428 - val_accuracy: 0.9251\n",
            "Epoch 25/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.4783 - accuracy: 0.8443 - val_loss: 0.2276 - val_accuracy: 0.9316\n",
            "Epoch 26/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.4680 - accuracy: 0.8467 - val_loss: 0.2311 - val_accuracy: 0.9295\n",
            "Epoch 27/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.4604 - accuracy: 0.8515 - val_loss: 0.2243 - val_accuracy: 0.9337\n",
            "Epoch 28/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.4555 - accuracy: 0.8527 - val_loss: 0.2254 - val_accuracy: 0.9316\n",
            "Epoch 29/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.4583 - accuracy: 0.8512 - val_loss: 0.2275 - val_accuracy: 0.9305\n",
            "Epoch 30/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.4479 - accuracy: 0.8556 - val_loss: 0.2327 - val_accuracy: 0.9294\n",
            "Epoch 31/198\n",
            "30/30 [==============================] - 1s 44ms/step - loss: 0.4320 - accuracy: 0.8589 - val_loss: 0.2369 - val_accuracy: 0.9268\n",
            "Epoch 32/198\n",
            "30/30 [==============================] - 2s 71ms/step - loss: 0.4273 - accuracy: 0.8610 - val_loss: 0.2312 - val_accuracy: 0.9305\n",
            "Epoch 33/198\n",
            "30/30 [==============================] - 1s 44ms/step - loss: 0.4226 - accuracy: 0.8636 - val_loss: 0.2334 - val_accuracy: 0.9291\n",
            "Epoch 34/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.4207 - accuracy: 0.8648 - val_loss: 0.2291 - val_accuracy: 0.9337\n",
            "Epoch 35/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.4171 - accuracy: 0.8654 - val_loss: 0.2211 - val_accuracy: 0.9339\n",
            "Epoch 36/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.4108 - accuracy: 0.8669 - val_loss: 0.2305 - val_accuracy: 0.9318\n",
            "Epoch 37/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.4040 - accuracy: 0.8703 - val_loss: 0.2362 - val_accuracy: 0.9274\n",
            "Epoch 38/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.4012 - accuracy: 0.8692 - val_loss: 0.2368 - val_accuracy: 0.9287\n",
            "Epoch 39/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3922 - accuracy: 0.8741 - val_loss: 0.2339 - val_accuracy: 0.9306\n",
            "Epoch 40/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.3931 - accuracy: 0.8753 - val_loss: 0.2249 - val_accuracy: 0.9331\n",
            "Epoch 41/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.3880 - accuracy: 0.8780 - val_loss: 0.2316 - val_accuracy: 0.9326\n",
            "Epoch 42/198\n",
            "30/30 [==============================] - 3s 111ms/step - loss: 0.3799 - accuracy: 0.8810 - val_loss: 0.2537 - val_accuracy: 0.9202\n",
            "Epoch 43/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3784 - accuracy: 0.8810 - val_loss: 0.2337 - val_accuracy: 0.9307\n",
            "Epoch 44/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3774 - accuracy: 0.8810 - val_loss: 0.2348 - val_accuracy: 0.9322\n",
            "Epoch 45/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3767 - accuracy: 0.8820 - val_loss: 0.2310 - val_accuracy: 0.9299\n",
            "Epoch 46/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.3646 - accuracy: 0.8863 - val_loss: 0.2289 - val_accuracy: 0.9358\n",
            "Epoch 47/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.3638 - accuracy: 0.8870 - val_loss: 0.2342 - val_accuracy: 0.9310\n",
            "Epoch 48/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3693 - accuracy: 0.8856 - val_loss: 0.2469 - val_accuracy: 0.9281\n",
            "Epoch 49/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.3626 - accuracy: 0.8852 - val_loss: 0.2408 - val_accuracy: 0.9295\n",
            "Epoch 50/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.3609 - accuracy: 0.8876 - val_loss: 0.2282 - val_accuracy: 0.9356\n",
            "Epoch 51/198\n",
            "30/30 [==============================] - 2s 54ms/step - loss: 0.3496 - accuracy: 0.8905 - val_loss: 0.2347 - val_accuracy: 0.9322\n",
            "Epoch 52/198\n",
            "30/30 [==============================] - 2s 68ms/step - loss: 0.3505 - accuracy: 0.8931 - val_loss: 0.2285 - val_accuracy: 0.9346\n",
            "Epoch 53/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.3541 - accuracy: 0.8904 - val_loss: 0.2368 - val_accuracy: 0.9329\n",
            "Epoch 54/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3460 - accuracy: 0.8932 - val_loss: 0.2530 - val_accuracy: 0.9242\n",
            "Epoch 55/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3395 - accuracy: 0.8931 - val_loss: 0.2325 - val_accuracy: 0.9351\n",
            "Epoch 56/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.3436 - accuracy: 0.8928 - val_loss: 0.2364 - val_accuracy: 0.9346\n",
            "Epoch 57/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3345 - accuracy: 0.8954 - val_loss: 0.2511 - val_accuracy: 0.9285\n",
            "Epoch 58/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.3348 - accuracy: 0.8944 - val_loss: 0.2458 - val_accuracy: 0.9310\n",
            "Epoch 59/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.3331 - accuracy: 0.8975 - val_loss: 0.2261 - val_accuracy: 0.9392\n",
            "Epoch 60/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.3342 - accuracy: 0.8951 - val_loss: 0.2256 - val_accuracy: 0.9378\n",
            "Epoch 61/198\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.3304 - accuracy: 0.8979 - val_loss: 0.2275 - val_accuracy: 0.9396\n",
            "Epoch 62/198\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.3332 - accuracy: 0.8970 - val_loss: 0.2340 - val_accuracy: 0.9367\n",
            "Epoch 63/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3214 - accuracy: 0.8995 - val_loss: 0.2497 - val_accuracy: 0.9311\n",
            "Epoch 64/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3263 - accuracy: 0.8988 - val_loss: 0.2320 - val_accuracy: 0.9382\n",
            "Epoch 65/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.3234 - accuracy: 0.8995 - val_loss: 0.2349 - val_accuracy: 0.9368\n",
            "Epoch 66/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3195 - accuracy: 0.9013 - val_loss: 0.2424 - val_accuracy: 0.9338\n",
            "Epoch 67/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3124 - accuracy: 0.9024 - val_loss: 0.2390 - val_accuracy: 0.9384\n",
            "Epoch 68/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3258 - accuracy: 0.9007 - val_loss: 0.2261 - val_accuracy: 0.9415\n",
            "Epoch 69/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3132 - accuracy: 0.9039 - val_loss: 0.2395 - val_accuracy: 0.9374\n",
            "Epoch 70/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.3086 - accuracy: 0.9035 - val_loss: 0.2350 - val_accuracy: 0.9393\n",
            "Epoch 71/198\n",
            "30/30 [==============================] - 2s 70ms/step - loss: 0.3109 - accuracy: 0.9030 - val_loss: 0.2375 - val_accuracy: 0.9362\n",
            "Epoch 72/198\n",
            "30/30 [==============================] - 2s 52ms/step - loss: 0.3087 - accuracy: 0.9043 - val_loss: 0.2395 - val_accuracy: 0.9377\n",
            "Epoch 73/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3066 - accuracy: 0.9036 - val_loss: 0.2361 - val_accuracy: 0.9386\n",
            "Epoch 74/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3066 - accuracy: 0.9049 - val_loss: 0.2431 - val_accuracy: 0.9365\n",
            "Epoch 75/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.3095 - accuracy: 0.9066 - val_loss: 0.2377 - val_accuracy: 0.9370\n",
            "Epoch 76/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2995 - accuracy: 0.9063 - val_loss: 0.2351 - val_accuracy: 0.9412\n",
            "Epoch 77/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2987 - accuracy: 0.9078 - val_loss: 0.2290 - val_accuracy: 0.9427\n",
            "Epoch 78/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.2999 - accuracy: 0.9084 - val_loss: 0.2313 - val_accuracy: 0.9414\n",
            "Epoch 79/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2998 - accuracy: 0.9071 - val_loss: 0.2306 - val_accuracy: 0.9429\n",
            "Epoch 80/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.2990 - accuracy: 0.9063 - val_loss: 0.2342 - val_accuracy: 0.9405\n",
            "Epoch 81/198\n",
            "30/30 [==============================] - 2s 64ms/step - loss: 0.2958 - accuracy: 0.9093 - val_loss: 0.2298 - val_accuracy: 0.9425\n",
            "Epoch 82/198\n",
            "30/30 [==============================] - 2s 61ms/step - loss: 0.2902 - accuracy: 0.9102 - val_loss: 0.2440 - val_accuracy: 0.9400\n",
            "Epoch 83/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.2929 - accuracy: 0.9089 - val_loss: 0.2339 - val_accuracy: 0.9426\n",
            "Epoch 84/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2937 - accuracy: 0.9112 - val_loss: 0.2360 - val_accuracy: 0.9412\n",
            "Epoch 85/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2881 - accuracy: 0.9099 - val_loss: 0.2362 - val_accuracy: 0.9430\n",
            "Epoch 86/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2898 - accuracy: 0.9108 - val_loss: 0.2450 - val_accuracy: 0.9394\n",
            "Epoch 87/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2923 - accuracy: 0.9107 - val_loss: 0.2372 - val_accuracy: 0.9421\n",
            "Epoch 88/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2831 - accuracy: 0.9134 - val_loss: 0.2266 - val_accuracy: 0.9455\n",
            "Epoch 89/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2949 - accuracy: 0.9109 - val_loss: 0.2315 - val_accuracy: 0.9424\n",
            "Epoch 90/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2861 - accuracy: 0.9129 - val_loss: 0.2427 - val_accuracy: 0.9390\n",
            "Epoch 91/198\n",
            "30/30 [==============================] - 2s 57ms/step - loss: 0.2885 - accuracy: 0.9114 - val_loss: 0.2343 - val_accuracy: 0.9436\n",
            "Epoch 92/198\n",
            "30/30 [==============================] - 2s 67ms/step - loss: 0.2851 - accuracy: 0.9117 - val_loss: 0.2365 - val_accuracy: 0.9424\n",
            "Epoch 93/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2841 - accuracy: 0.9121 - val_loss: 0.2256 - val_accuracy: 0.9460\n",
            "Epoch 94/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2785 - accuracy: 0.9141 - val_loss: 0.2365 - val_accuracy: 0.9439\n",
            "Epoch 95/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2816 - accuracy: 0.9148 - val_loss: 0.2281 - val_accuracy: 0.9453\n",
            "Epoch 96/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2798 - accuracy: 0.9145 - val_loss: 0.2437 - val_accuracy: 0.9391\n",
            "Epoch 97/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2768 - accuracy: 0.9135 - val_loss: 0.2395 - val_accuracy: 0.9424\n",
            "Epoch 98/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2782 - accuracy: 0.9141 - val_loss: 0.2260 - val_accuracy: 0.9460\n",
            "Epoch 99/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2728 - accuracy: 0.9146 - val_loss: 0.2458 - val_accuracy: 0.9404\n",
            "Epoch 100/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2745 - accuracy: 0.9159 - val_loss: 0.2397 - val_accuracy: 0.9442\n",
            "Epoch 101/198\n",
            "30/30 [==============================] - 2s 51ms/step - loss: 0.2716 - accuracy: 0.9167 - val_loss: 0.2318 - val_accuracy: 0.9444\n",
            "Epoch 102/198\n",
            "30/30 [==============================] - 2s 69ms/step - loss: 0.2772 - accuracy: 0.9171 - val_loss: 0.2372 - val_accuracy: 0.9454\n",
            "Epoch 103/198\n",
            "30/30 [==============================] - 1s 45ms/step - loss: 0.2758 - accuracy: 0.9159 - val_loss: 0.2275 - val_accuracy: 0.9468\n",
            "Epoch 104/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2706 - accuracy: 0.9169 - val_loss: 0.2267 - val_accuracy: 0.9484\n",
            "Epoch 105/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2714 - accuracy: 0.9175 - val_loss: 0.2352 - val_accuracy: 0.9453\n",
            "Epoch 106/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2680 - accuracy: 0.9179 - val_loss: 0.2363 - val_accuracy: 0.9448\n",
            "Epoch 107/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2707 - accuracy: 0.9172 - val_loss: 0.2392 - val_accuracy: 0.9443\n",
            "Epoch 108/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2688 - accuracy: 0.9168 - val_loss: 0.2445 - val_accuracy: 0.9424\n",
            "Epoch 109/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2672 - accuracy: 0.9178 - val_loss: 0.2382 - val_accuracy: 0.9440\n",
            "Epoch 110/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2663 - accuracy: 0.9189 - val_loss: 0.2404 - val_accuracy: 0.9443\n",
            "Epoch 111/198\n",
            "30/30 [==============================] - 1s 46ms/step - loss: 0.2629 - accuracy: 0.9190 - val_loss: 0.2305 - val_accuracy: 0.9462\n",
            "Epoch 112/198\n",
            "30/30 [==============================] - 2s 70ms/step - loss: 0.2616 - accuracy: 0.9195 - val_loss: 0.2244 - val_accuracy: 0.9489\n",
            "Epoch 113/198\n",
            "30/30 [==============================] - 1s 45ms/step - loss: 0.2621 - accuracy: 0.9191 - val_loss: 0.2307 - val_accuracy: 0.9487\n",
            "Epoch 114/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2641 - accuracy: 0.9193 - val_loss: 0.2448 - val_accuracy: 0.9436\n",
            "Epoch 115/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2570 - accuracy: 0.9204 - val_loss: 0.2360 - val_accuracy: 0.9459\n",
            "Epoch 116/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2558 - accuracy: 0.9219 - val_loss: 0.2330 - val_accuracy: 0.9468\n",
            "Epoch 117/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2569 - accuracy: 0.9215 - val_loss: 0.2336 - val_accuracy: 0.9470\n",
            "Epoch 118/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2590 - accuracy: 0.9221 - val_loss: 0.2315 - val_accuracy: 0.9488\n",
            "Epoch 119/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2590 - accuracy: 0.9207 - val_loss: 0.2326 - val_accuracy: 0.9481\n",
            "Epoch 120/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2565 - accuracy: 0.9213 - val_loss: 0.2314 - val_accuracy: 0.9477\n",
            "Epoch 121/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.2608 - accuracy: 0.9199 - val_loss: 0.2357 - val_accuracy: 0.9463\n",
            "Epoch 122/198\n",
            "30/30 [==============================] - 2s 66ms/step - loss: 0.2607 - accuracy: 0.9200 - val_loss: 0.2454 - val_accuracy: 0.9449\n",
            "Epoch 123/198\n",
            "30/30 [==============================] - 2s 56ms/step - loss: 0.2571 - accuracy: 0.9226 - val_loss: 0.2405 - val_accuracy: 0.9468\n",
            "Epoch 124/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2546 - accuracy: 0.9233 - val_loss: 0.2388 - val_accuracy: 0.9498\n",
            "Epoch 125/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2612 - accuracy: 0.9194 - val_loss: 0.2375 - val_accuracy: 0.9483\n",
            "Epoch 126/198\n",
            "30/30 [==============================] - 1s 37ms/step - loss: 0.2574 - accuracy: 0.9201 - val_loss: 0.2506 - val_accuracy: 0.9454\n",
            "Epoch 127/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2480 - accuracy: 0.9224 - val_loss: 0.2378 - val_accuracy: 0.9498\n",
            "Epoch 128/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2515 - accuracy: 0.9228 - val_loss: 0.2368 - val_accuracy: 0.9474\n",
            "Epoch 129/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2493 - accuracy: 0.9240 - val_loss: 0.2352 - val_accuracy: 0.9473\n",
            "Epoch 130/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2507 - accuracy: 0.9228 - val_loss: 0.2354 - val_accuracy: 0.9492\n",
            "Epoch 131/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2540 - accuracy: 0.9243 - val_loss: 0.2345 - val_accuracy: 0.9468\n",
            "Epoch 132/198\n",
            "30/30 [==============================] - 2s 63ms/step - loss: 0.2505 - accuracy: 0.9240 - val_loss: 0.2386 - val_accuracy: 0.9472\n",
            "Epoch 133/198\n",
            "30/30 [==============================] - 2s 63ms/step - loss: 0.2443 - accuracy: 0.9249 - val_loss: 0.2380 - val_accuracy: 0.9474\n",
            "Epoch 134/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2500 - accuracy: 0.9236 - val_loss: 0.2372 - val_accuracy: 0.9494\n",
            "Epoch 135/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2475 - accuracy: 0.9251 - val_loss: 0.2368 - val_accuracy: 0.9501\n",
            "Epoch 136/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2464 - accuracy: 0.9253 - val_loss: 0.2384 - val_accuracy: 0.9495\n",
            "Epoch 137/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2436 - accuracy: 0.9245 - val_loss: 0.2394 - val_accuracy: 0.9478\n",
            "Epoch 138/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2464 - accuracy: 0.9241 - val_loss: 0.2363 - val_accuracy: 0.9493\n",
            "Epoch 139/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2447 - accuracy: 0.9243 - val_loss: 0.2328 - val_accuracy: 0.9517\n",
            "Epoch 140/198\n",
            "30/30 [==============================] - 1s 44ms/step - loss: 0.2450 - accuracy: 0.9257 - val_loss: 0.2378 - val_accuracy: 0.9500\n",
            "Epoch 141/198\n",
            "30/30 [==============================] - 2s 74ms/step - loss: 0.2419 - accuracy: 0.9274 - val_loss: 0.2396 - val_accuracy: 0.9493\n",
            "Epoch 142/198\n",
            "30/30 [==============================] - 3s 106ms/step - loss: 0.2401 - accuracy: 0.9265 - val_loss: 0.2401 - val_accuracy: 0.9485\n",
            "Epoch 143/198\n",
            "30/30 [==============================] - 2s 73ms/step - loss: 0.2415 - accuracy: 0.9259 - val_loss: 0.2336 - val_accuracy: 0.9514\n",
            "Epoch 144/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2455 - accuracy: 0.9239 - val_loss: 0.2361 - val_accuracy: 0.9498\n",
            "Epoch 145/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2468 - accuracy: 0.9247 - val_loss: 0.2427 - val_accuracy: 0.9460\n",
            "Epoch 146/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2401 - accuracy: 0.9266 - val_loss: 0.2225 - val_accuracy: 0.9526\n",
            "Epoch 147/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2389 - accuracy: 0.9270 - val_loss: 0.2253 - val_accuracy: 0.9516\n",
            "Epoch 148/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.2406 - accuracy: 0.9278 - val_loss: 0.2296 - val_accuracy: 0.9498\n",
            "Epoch 149/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2385 - accuracy: 0.9273 - val_loss: 0.2374 - val_accuracy: 0.9498\n",
            "Epoch 150/198\n",
            "30/30 [==============================] - 2s 54ms/step - loss: 0.2431 - accuracy: 0.9256 - val_loss: 0.2341 - val_accuracy: 0.9511\n",
            "Epoch 151/198\n",
            "30/30 [==============================] - 2s 70ms/step - loss: 0.2396 - accuracy: 0.9276 - val_loss: 0.2471 - val_accuracy: 0.9470\n",
            "Epoch 152/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.2370 - accuracy: 0.9272 - val_loss: 0.2430 - val_accuracy: 0.9477\n",
            "Epoch 153/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2457 - accuracy: 0.9253 - val_loss: 0.2493 - val_accuracy: 0.9442\n",
            "Epoch 154/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2366 - accuracy: 0.9285 - val_loss: 0.2521 - val_accuracy: 0.9468\n",
            "Epoch 155/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2423 - accuracy: 0.9264 - val_loss: 0.2357 - val_accuracy: 0.9489\n",
            "Epoch 156/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2408 - accuracy: 0.9268 - val_loss: 0.2453 - val_accuracy: 0.9479\n",
            "Epoch 157/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2376 - accuracy: 0.9269 - val_loss: 0.2465 - val_accuracy: 0.9456\n",
            "Epoch 158/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2371 - accuracy: 0.9275 - val_loss: 0.2417 - val_accuracy: 0.9450\n",
            "Epoch 159/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2346 - accuracy: 0.9295 - val_loss: 0.2299 - val_accuracy: 0.9507\n",
            "Epoch 160/198\n",
            "30/30 [==============================] - 2s 54ms/step - loss: 0.2353 - accuracy: 0.9281 - val_loss: 0.2372 - val_accuracy: 0.9509\n",
            "Epoch 161/198\n",
            "30/30 [==============================] - 2s 70ms/step - loss: 0.2344 - accuracy: 0.9286 - val_loss: 0.2448 - val_accuracy: 0.9487\n",
            "Epoch 162/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2346 - accuracy: 0.9291 - val_loss: 0.2486 - val_accuracy: 0.9474\n",
            "Epoch 163/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2339 - accuracy: 0.9294 - val_loss: 0.2390 - val_accuracy: 0.9474\n",
            "Epoch 164/198\n",
            "30/30 [==============================] - 1s 38ms/step - loss: 0.2369 - accuracy: 0.9283 - val_loss: 0.2390 - val_accuracy: 0.9487\n",
            "Epoch 165/198\n",
            "30/30 [==============================] - 2s 71ms/step - loss: 0.2313 - accuracy: 0.9303 - val_loss: 0.2395 - val_accuracy: 0.9482\n",
            "Epoch 166/198\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 0.2355 - accuracy: 0.9278 - val_loss: 0.2457 - val_accuracy: 0.9503\n",
            "Epoch 167/198\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 0.2336 - accuracy: 0.9298 - val_loss: 0.2353 - val_accuracy: 0.9510\n",
            "Epoch 168/198\n",
            "30/30 [==============================] - 2s 69ms/step - loss: 0.2352 - accuracy: 0.9284 - val_loss: 0.2456 - val_accuracy: 0.9496\n",
            "Epoch 169/198\n",
            "30/30 [==============================] - 2s 50ms/step - loss: 0.2336 - accuracy: 0.9288 - val_loss: 0.2376 - val_accuracy: 0.9511\n",
            "Epoch 170/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2252 - accuracy: 0.9310 - val_loss: 0.2390 - val_accuracy: 0.9488\n",
            "Epoch 171/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.2329 - accuracy: 0.9309 - val_loss: 0.2409 - val_accuracy: 0.9485\n",
            "Epoch 172/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2320 - accuracy: 0.9297 - val_loss: 0.2490 - val_accuracy: 0.9478\n",
            "Epoch 173/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2332 - accuracy: 0.9300 - val_loss: 0.2353 - val_accuracy: 0.9497\n",
            "Epoch 174/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.2278 - accuracy: 0.9308 - val_loss: 0.2429 - val_accuracy: 0.9512\n",
            "Epoch 175/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2273 - accuracy: 0.9312 - val_loss: 0.2358 - val_accuracy: 0.9529\n",
            "Epoch 176/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2291 - accuracy: 0.9304 - val_loss: 0.2381 - val_accuracy: 0.9482\n",
            "Epoch 177/198\n",
            "30/30 [==============================] - 1s 45ms/step - loss: 0.2284 - accuracy: 0.9304 - val_loss: 0.2415 - val_accuracy: 0.9502\n",
            "Epoch 178/198\n",
            "30/30 [==============================] - 2s 69ms/step - loss: 0.2247 - accuracy: 0.9321 - val_loss: 0.2574 - val_accuracy: 0.9456\n",
            "Epoch 179/198\n",
            "30/30 [==============================] - 2s 53ms/step - loss: 0.2262 - accuracy: 0.9314 - val_loss: 0.2546 - val_accuracy: 0.9492\n",
            "Epoch 180/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2247 - accuracy: 0.9316 - val_loss: 0.2529 - val_accuracy: 0.9478\n",
            "Epoch 181/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2253 - accuracy: 0.9312 - val_loss: 0.2476 - val_accuracy: 0.9490\n",
            "Epoch 182/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2242 - accuracy: 0.9312 - val_loss: 0.2370 - val_accuracy: 0.9514\n",
            "Epoch 183/198\n",
            "30/30 [==============================] - 1s 41ms/step - loss: 0.2217 - accuracy: 0.9324 - val_loss: 0.2448 - val_accuracy: 0.9499\n",
            "Epoch 184/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2255 - accuracy: 0.9316 - val_loss: 0.2355 - val_accuracy: 0.9514\n",
            "Epoch 185/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2245 - accuracy: 0.9313 - val_loss: 0.2427 - val_accuracy: 0.9501\n",
            "Epoch 186/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2222 - accuracy: 0.9332 - val_loss: 0.2452 - val_accuracy: 0.9502\n",
            "Epoch 187/198\n",
            "30/30 [==============================] - 1s 46ms/step - loss: 0.2257 - accuracy: 0.9322 - val_loss: 0.2452 - val_accuracy: 0.9498\n",
            "Epoch 188/198\n",
            "30/30 [==============================] - 2s 72ms/step - loss: 0.2225 - accuracy: 0.9319 - val_loss: 0.2513 - val_accuracy: 0.9499\n",
            "Epoch 189/198\n",
            "30/30 [==============================] - 1s 47ms/step - loss: 0.2222 - accuracy: 0.9327 - val_loss: 0.2477 - val_accuracy: 0.9497\n",
            "Epoch 190/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2141 - accuracy: 0.9345 - val_loss: 0.2482 - val_accuracy: 0.9494\n",
            "Epoch 191/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2187 - accuracy: 0.9340 - val_loss: 0.2342 - val_accuracy: 0.9540\n",
            "Epoch 192/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2205 - accuracy: 0.9330 - val_loss: 0.2388 - val_accuracy: 0.9504\n",
            "Epoch 193/198\n",
            "30/30 [==============================] - 1s 40ms/step - loss: 0.2236 - accuracy: 0.9326 - val_loss: 0.2417 - val_accuracy: 0.9524\n",
            "Epoch 194/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2201 - accuracy: 0.9337 - val_loss: 0.2479 - val_accuracy: 0.9487\n",
            "Epoch 195/198\n",
            "30/30 [==============================] - 1s 39ms/step - loss: 0.2205 - accuracy: 0.9325 - val_loss: 0.2576 - val_accuracy: 0.9470\n",
            "Epoch 196/198\n",
            "30/30 [==============================] - 1s 42ms/step - loss: 0.2233 - accuracy: 0.9324 - val_loss: 0.2457 - val_accuracy: 0.9482\n",
            "Epoch 197/198\n",
            "30/30 [==============================] - 1s 49ms/step - loss: 0.2186 - accuracy: 0.9345 - val_loss: 0.2436 - val_accuracy: 0.9517\n",
            "Epoch 198/198\n",
            "30/30 [==============================] - 2s 67ms/step - loss: 0.2159 - accuracy: 0.9351 - val_loss: 0.2496 - val_accuracy: 0.9485\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))  # Training the model and validating on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-zICuni1d34",
        "outputId": "b4a174a8-af0f-447a-8ab5-181e27b4777e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 12ms/step - loss: 0.2496 - accuracy: 0.9485\n",
            "Test loss: 0.24963341653347015\n",
            "Test accuracy: 0.9484999775886536\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=1, batch_size=batch_size)  # Evaluating the model on the test data.  (verbose pour l'affichage)   (evaluate pour l'evaluation du model apres l'apprentissage)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BmGNzdx4qkz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCA1W6sR311D"
      },
      "outputs": [],
      "source": [
        "# Plot training accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "# Plot validation accuracy\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pour evaluer la performance on doit :\n",
        "- comparer l'apprentissage et test\n",
        "\n",
        "*au cours de lapprentissage est ce que le model est en train d'amiliorer ou non (accuraccy temchi w tzid wala bel 3aks)\n",
        "\n",
        "*overfiting or underfiting or good model or underfiting and overfiting at the same time\n",
        "\n",
        "*overfitting : si les courbes de valisation (loss or accuracy or both ) de degrade , accuracy tih w loss tzid , ecart mabin apprentissage et test\n",
        "\n",
        "*les causes de overfitting :\n",
        "- beaucoup d'epochs (solution: diminuer les epochs)\n",
        "- un reseau (architecture) complexe et profond par rapport au data (solution: alleger l'architecture : reduire le nbr des neurons de chaque  couche cach√©e ou augmenter la valeur de dropout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7MeEtHPw74Em"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ykhpx2m38z7"
      },
      "outputs": [],
      "source": [
        "# Summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialisation :\n",
        "   - K = 2 (nombre de clusters)\n",
        "   - Centres initiaux : c1(x=0, y=1, z=0) et c2(x=1, y=0, z=0)\n",
        "\n",
        "2. It√©ration 1 :\n",
        "   - Calcul des distances :\n",
        "     - Pour p1 : distance(p1, c1) = sqrt((0-0)^2 + (1-0)^2 + (0-0)^2) = 1\n",
        "     - Pour p1 : distance(p1, c2) = sqrt((0-1)^2 + (1-0)^2 + (0-0)^2) = sqrt(2)\n",
        "     - Pour p2 : distance(p2, c1) = sqrt((1-0)^2 + (1-1)^2 + (0-0)^2) = 1\n",
        "     - Pour p2 : distance(p2, c2) = sqrt((1-1)^2 + (1-0)^2 + (0-0)^2) = 1\n",
        "     - Pour p3 : distance(p3, c1) = sqrt((0-0)^2 + (-1-1)^2 + (-1-0)^2) = sqrt(5)\n",
        "     - Pour p3 : distance(p3, c2) = sqrt((0-1)^2 + (-1-0)^2 + (-1-0)^2) = sqrt(3)\n",
        "     - Pour p4 : distance(p4, c1) = sqrt((1-0)^2 + (1-1)^2 + (1-0)^2) = sqrt(2)\n",
        "     - Pour p4 : distance(p4, c2) = sqrt((1-1)^2 + (1-0)^2 + (1-0)^2) = 1\n",
        "     - Pour p5 : distance(p5, c1) = sqrt((-1-0)^2 + (-1-1)^2 + (-1-0)^2) = sqrt(5)\n",
        "     - Pour p5 : distance(p5, c2) = sqrt((-1-1)^2 + (-1-0)^2 + (-1-0)^2) = sqrt(3)\n",
        "     - Pour p6 : distance(p6, c1) = sqrt((0-0)^2 + (1-1)^2 + (0-0)^2) = 0\n",
        "     - Pour p6 : distance(p6, c2) = sqrt((0-1)^2 + (1-0)^2 + (0-0)^2) = sqrt\n",
        "\n",
        "(2)\n",
        "\n",
        "   - Assignation des points aux clusters :\n",
        "     - Cluster 1 : {p1, p2, p4, p6}\n",
        "     - Cluster 2 : {p3, p5}\n",
        "\n",
        "   - Calcul des nouveaux centres des clusters :\n",
        "     - Centre du Cluster 1 : (x=0.5, y=0.75, z=0)\n",
        "     - Centre du Cluster 2 : (x=0, y=0, z=-0.5)\n",
        "\n",
        "Poursuivons avec l'it√©ration 2 de l'algorithme :\n",
        "\n",
        "1. Utilisation des nouveaux centres des clusters de l'it√©ration pr√©c√©dente :\n",
        "   - Centre du Cluster 1 : (x=0.5, y=0.75, z=0)\n",
        "   - Centre du Cluster 2 : (x=0, y=0, z=-0.5)\n",
        "\n",
        "2. Calcul des distances entre chaque point et les nouveaux centres des clusters :\n",
        "   - Pour p1 : distance(p1, c1) = sqrt((0-0.5)^2 + (1-0.75)^2 + (0-0)^2) = sqrt(0.125)\n",
        "   - Pour p1 : distance(p1, c2) = sqrt((0-0)^2 + (1-0)^2 + (0+0.5)^2) = sqrt(0.5)\n",
        "   - Pour p2 : distance(p2, c1) = sqrt((1-0.5)^2 + (1-0.75)^2 + (0-0)^2) = sqrt(0.125)\n",
        "   - Pour p2 : distance(p2, c2) = sqrt((1-0)^2 + (1-0)^2 + (0+0.5)^2) = sqrt(1.5)\n",
        "   - Pour p3 : distance(p3, c1) = sqrt((0-0.5)^2 + (-1-0.75)^2 + (-1-0)^2) = sqrt(4.125)\n",
        "   - Pour p3 : distance(p3, c2) = sqrt((0-0)^2 + (-1-0)^2 + (-1+0.5)^2) = sqrt(1.75)\n",
        "   - Pour p4 : distance(p4, c1) = sqrt((1-0.5)^2 + (1-0.75)^2 + (1-0)^2) = sqrt(0.125)\n",
        "   - Pour p4 : distance(p4, c2) = sqrt((1-0)^2 + (1-0)^2 + (1+0.5)^2) = sqrt(3.5)\n",
        "   - Pour p5 : distance(p5, c1) = sqrt((-1-0.5)^2 + (-1-0.75)^2 + (-1-0)^2) = sqrt(4.125)\n",
        "   - Pour p5 : distance(p5, c2) = sqrt((-1-0)^2 + (-1-0)^2 + (-1+0.5)^2) = sqrt(1.75)\n",
        "   - Pour p6 : distance(p6, c1) = sqrt((0-0.5)^2 + (1-0.75)^2 + (0-0)^2) = sqrt(0.125)\n",
        "   - Pour p6 : distance(p6, c2) = sqrt((0-0)^2 + (1-0)^2 + (0+0.5)^2) = sqrt(0.5)\n",
        "\n",
        "3. Assignation des points aux clusters en fonction des distances minimales :\n",
        "   - Cluster 1 : {p1, p2, p4, p6}\n",
        "   - Cluster 2 : {p3, p5}\n",
        "\n",
        "4. Calcul des nouveaux centres des clusters en prenant la moyenne des coordonn√©es des points de chaque cluster :\n",
        "   - Centre du Cluster 1 : (x=0.5, y=0.75, z=0)\n",
        "   - Centre du Cluster 2 : (x=0, y=0, z=-0\n",
        "\n",
        ".5)\n",
        "\n",
        "√Ä ce stade, les centres des clusters n'ont pas chang√© significativement par rapport √† l'it√©ration pr√©c√©dente. Vous pouvez soit arr√™ter l'algorithme √† ce stade car les centres des clusters ont converg√©, soit r√©p√©ter les √©tapes 2 √† 4 jusqu'√† ce que les centres des clusters convergent davantage ou jusqu'√† atteindre un certain nombre maximum d'it√©rations pr√©d√©fini.\n",
        "\n",
        "Pour pr√©dire dans quel groupe la personne p7 (x=1, y=-1, z=0) va appartenir, nous devons utiliser les centres des clusters obtenus apr√®s l'ach√®vement de l'algorithme k-means.\n",
        "\n",
        "Supposons que les centres des clusters √† la fin de l'algorithme soient les suivants :\n",
        "\n",
        "Centre du Cluster 1 : (x=0.5, y=0.75, z=0)\n",
        "Centre du Cluster 2 : (x=0, y=0, z=-0.5)\n",
        "Calculons la distance entre p7 et les centres des clusters :\n",
        "\n",
        "Pour p7 : distance(p7, c1) = sqrt((1-0.5)^2 + (-1-0.75)^2 + (0-0)^2) = sqrt(2.125)\n",
        "Pour p7 : distance(p7, c2) = sqrt((1-0)^2 + (-1-0)^2 + (0+0.5)^2) = sqrt(1.75)\n",
        "La distance entre p7 et le centre du Cluster 2 (sqrt(1.75)) est plus petite que la distance entre p7 et le centre du Cluster 1 (sqrt(2.125)). Par cons√©quent, p7 sera attribu√© au Cluster 2.\n",
        "\n",
        "Donc, selon la pr√©diction, la personne p7 (x=1, y=-1, z=0) appartiendrait au Cluster 2.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ps5HtEEOEky5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}